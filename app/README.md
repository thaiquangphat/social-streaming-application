# ğŸš€ Reddit Prefect Producer

> **Stream trending Reddit content to Kafka with orchestrated reliability**

A production-ready data ingestion pipeline that discovers trending Reddit content and publishes structured events to Apache Kafka, orchestrated by Prefect for reliability and observability.

---

## ğŸ¯ What It Does

```
Reddit API â†’ Prefect Tasks â†’ Kafka Topics â†’ Your Data Pipeline
   ğŸ“¡            âš™ï¸              ğŸ“¨              ğŸ”®
```

- **Discovers** trending subreddits and hot content using Reddit's public API
- **Normalizes** posts and comments into strongly-typed data models
- **Publishes** events to Kafka with retry logic and delivery guarantees
- **Orchestrates** execution with Prefect for monitoring and scheduling
- **Tracks** progress with watermarking to avoid duplicate processing

---

## ğŸ—ï¸ Architecture

### System Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Orchestration Layer                â”‚
â”‚            (Prefect Flow)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Task Layer                      â”‚
â”‚   (Rate Limiting, Logic)     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reddit Client â”‚         â”‚ Kafka Publisher  â”‚
â”‚   (+ Retry)   â”‚         â”‚  (+ Callbacks)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Purpose | Location |
|-----------|---------|----------|
| ğŸŒ **Reddit Client** | Fetches trending content with retry logic | `app/clients/reddit_client.py` |
| ğŸ“Š **Schema Models** | Pydantic models for type safety | `app/clients/schema.py` |
| ğŸ“¤ **Kafka Publisher** | Publishes with delivery callbacks | `app/kafka/publisher.py` |
| âš¡ **Ingestion Task** | Core fetch-publish loop | `app/task/reddit.py` |
| ğŸ­ **Prefect Flow** | Orchestration and dependency wiring | `app/flow.py` |
| âš™ï¸ **Configuration** | Environment-driven settings | `app/config.py` |

---

## ğŸ“¦ Data Model

### Message Envelope

Every event is wrapped in a consistent envelope structure:

```json
{
  "entity_type": "reddit_submission",
  "source": "reddit",
  "mode": "trending",
  "emitted_at": "2025-01-01T12:00:00+00:00",
  "payload": { /* event data */ },
  "metadata": {
    "subreddit": "python",
    "post_sort": "hot"
  }
}
```

### Event Types

#### ğŸ“ Submission Event
```json
{
  "id": "abc123",
  "subreddit": "python",
  "author": "someuser",
  "title": "Interesting post",
  "body": "Post content...",
  "created_utc": "2025-01-01T11:59:00+00:00",
  "score": 100,
  "num_comments": 5,
  "url": "https://reddit.com/...",
  "permalink": "/r/python/comments/abc123/...",
  "flair": "Discussion"
}
```

#### ğŸ’¬ Comment Event
Similar structure with parent linkage and comment-specific fields.

---

## ğŸ¨ Architectural Patterns

- **ğŸ§© Layered Architecture** - Clear separation of concerns
- **ğŸ“® Envelope Pattern** - Consistent message structure
- **ğŸŒŠ Event-Driven Streaming** - Decoupled producer/consumer
- **ğŸ”„ Retry with Exponential Backoff** - Resilience to failures
- **ğŸ’‰ Dependency Injection** - Testable and maintainable
- **ğŸ“‹ Configuration as Code** - Type-safe environment settings

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file from `.env.example`:

#### Reddit Settings
```bash
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_secret
REDDIT_USER_AGENT=MyApp/1.0
REDDIT_SUBREDDIT_LIMIT=5          # Trending subreddits to fetch
REDDIT_POSTS_PER_SUBREDDIT=10     # Posts per subreddit
REDDIT_COMMENT_LIMIT=20           # Comments per post (0=disabled)
REDDIT_POST_SORT=hot              # hot, new, top, or rising
```

#### Kafka Settings
```bash
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_CLIENT_ID=reddit-producer
TOPIC_REDDIT_SUBMISSIONS=reddit.submissions
TOPIC_REDDIT_COMMENTS=reddit.comments
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies
uv sync
```

### Run Locally

```bash
# Execute the flow
uv run app/flow.py
```

### Validate with Consumer

```bash
# Run the reference consumer to see messages
uv run consumer/consumer.py
```

---

## ğŸ”„ Control Flow

```
1. Load Configuration
        â†“
2. Initialize Clients (Reddit + Kafka)
        â†“
3. Discover Trending Subreddits
        â†“
4. For Each Subreddit:
   â”œâ”€ Fetch Posts (sorted by hot/new/top/rising)
   â”œâ”€ Filter by Watermark
   â”œâ”€ Serialize to Submission Event
   â”œâ”€ Wrap in Envelope
   â”œâ”€ Publish to Kafka
   â””â”€ For Each Post:
      â”œâ”€ Fetch Comments (limited)
      â”œâ”€ Serialize to Comment Events
      â””â”€ Publish to Kafka
        â†“
5. Flush & Close Publisher
```

---

## ğŸ›¡ï¸ Reliability Features

### Error Handling

| Error Type | Strategy | Implementation |
|------------|----------|----------------|
| ğŸŒ Reddit API Failures | Exponential backoff retry | `tenacity` with max attempts |
| ğŸ“¤ Kafka Publish Failures | Delivery callbacks + retry | Retriable vs. fatal error classification |
| ğŸ’¾ Buffer Full | Backpressure detection | Treated as retriable, automatic retry |

### Observability

- **ğŸ“Š Prefect Logs** - Flow and task execution tracking
- **ğŸ” Structured Logging** - Event metadata and errors
- **âœ… Delivery Callbacks** - Per-message confirmation
- **ğŸ·ï¸ Consumer Validation** - Reference implementation for testing

---

## ğŸ›ï¸ Operations & Tuning

### Rate Limiting

Conservative sleep intervals between API calls respect Reddit's public API limits. Adjust in `app/task/reddit.py` as needed.

### Throughput

Scale up ingestion by increasing:
- `REDDIT_SUBREDDIT_LIMIT` - More subreddits
- `REDDIT_POSTS_PER_SUBREDDIT` - More posts per subreddit
- `REDDIT_COMMENT_LIMIT` - More comments per post

âš ï¸ Ensure Kafka and downstream systems can handle the increased load.
---

## ğŸ”§ Extensibility

### Add New Event Types

1. Define Pydantic model in `app/clients/schema.py`
2. Extend envelope builder in `app/kafka/publisher.py`
3. Route events through the task

### Add New Sources

1. Create client module (similar to `RedditClient`)
2. Use same envelope structure
3. Add dedicated task or extend existing one

### Custom Metadata & Headers

The publisher accepts headers and metadata for:
- Downstream routing
- Filtering and indexing
- Audit trails

---

## ğŸ“ Project Structure

```
reddit-prefect-producer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ flow.py                  # ğŸ­ Prefect flow definition
â”‚   â”œâ”€â”€ config.py                # âš™ï¸ Configuration management
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ reddit_client.py     # ğŸŒ Reddit API wrapper
â”‚   â”‚   â””â”€â”€ schema.py            # ğŸ“Š Pydantic models
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ publisher.py         # ğŸ“¤ Kafka publishing logic
â”‚   â””â”€â”€ task/
â”‚       â””â”€â”€ reddit.py            # âš¡ Main ingestion task
â”‚       
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer.py              # âœ… Reference consumer
â”œâ”€â”€ .env.example                 # ğŸ“‹ Configuration template
â””â”€â”€ README.md                    # ğŸ“– This file
```

---

## âš ï¸ Limitations & Considerations

### Ordering
Kafka preserves order **per partition**, not globally. Events for the same key remain ordered within a partition.

### Delivery Semantics
- **Current**: At-least-once delivery
- **Exactly-once**: Requires additional Kafka configuration (idempotent producer, transactions)

### API Variability
Reddit's public API may change or enforce rate limits. Retry policies mitigate but don't eliminate this risk.

---



## ğŸ“š Additional Resources

- [Prefect Documentation](https://docs.prefect.io/)
- [Confluent Kafka Python](https://docs.confluent.io/kafka-clients/python/current/overview.html)
- [PRAW (Reddit API)](https://praw.readthedocs.io/)
- [Pydantic](https://docs.pydantic.dev/)

---
